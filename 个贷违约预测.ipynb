{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd867301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import re\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, average_precision_score\n",
    "from sklearn.model_selection import KFold\n",
    "from lightgbm import LGBMClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from dateutil.relativedelta import relativedelta\n",
    "train_data = pd.read_csv('./train_public.csv')\n",
    "submit_example = pd.read_csv('./submit_example.csv')\n",
    "test_public = pd.read_csv('./test_public.csv')\n",
    "train_inte = pd.read_csv('./train_internet.csv')\n",
    "\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_rows', 200)\n",
    "pd.set_option('float_format', lambda x: '%.3f' % x)\n",
    "def train_model(data_, test_, y_, folds_):\n",
    "    oof_preds = np.zeros(data_.shape[0])\n",
    "    sub_preds = np.zeros(test_.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    feats = [f for f in data_.columns if f not in ['loan_id', 'user_id', 'isDefault'] ]\n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(folds_.split(data_)):\n",
    "        trn_x, trn_y = data_[feats].iloc[trn_idx], y_.iloc[trn_idx]\n",
    "        val_x, val_y = data_[feats].iloc[val_idx], y_.iloc[val_idx]\n",
    "        clf = LGBMClassifier(\n",
    "            n_estimators=4000,\n",
    "            learning_rate=0.08,\n",
    "            num_leaves=2**5,\n",
    "            colsample_bytree=.65,\n",
    "            subsample=.9,\n",
    "            max_depth=5,\n",
    "#             max_bin=250,\n",
    "            reg_alpha=.3,\n",
    "            reg_lambda=.3,\n",
    "            min_split_gain=.01,\n",
    "            min_child_weight=2,\n",
    "            silent=-1,\n",
    "            verbose=-1,\n",
    "        )\n",
    "        \n",
    "        clf.fit(trn_x, trn_y, \n",
    "                eval_set= [(trn_x, trn_y), (val_x, val_y)], \n",
    "                eval_metric='auc', verbose=100, early_stopping_rounds=40  #30\n",
    "               )\n",
    "\n",
    "        oof_preds[val_idx] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "        sub_preds += clf.predict_proba(test_[feats], num_iteration=clf.best_iteration_)[:, 1] / folds_.n_splits\n",
    "        \n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        \n",
    "        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(val_y, oof_preds[val_idx])))\n",
    "        del clf, trn_x, trn_y, val_x, val_y\n",
    "        gc.collect()\n",
    "        \n",
    "    print('Full AUC score %.6f' % roc_auc_score(y, oof_preds)) \n",
    "    \n",
    "    test_['isDefault'] = sub_preds\n",
    "\n",
    "    return oof_preds, test_[['loan_id', 'isDefault']], feature_importance_df\n",
    "    \n",
    "def display_importances(feature_importance_df_):\n",
    "    # Plot feature importances\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "        by=\"importance\", ascending=False)[:50].index\n",
    "    \n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "    \n",
    "    plt.figure(figsize=(8,10))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", \n",
    "                data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lgbm_importances.png')\n",
    "\n",
    "def workYearDIc(x):\n",
    "    if str(x)=='nan':\n",
    "        return -1\n",
    "    x = x.replace('< 1','0')\n",
    "    return int(re.search('(\\d+)', x).group())\n",
    "\n",
    "def findDig(val):\n",
    "    fd = re.search('(\\d+-)', val)\n",
    "    if fd is None:\n",
    "        return '1-'+val\n",
    "    return val + '-01'\n",
    "\n",
    "\n",
    "class_dict = {\n",
    "    'A': 1,\n",
    "    'B': 2,\n",
    "    'C': 3,\n",
    "    'D': 4,\n",
    "    'E': 5,\n",
    "    'F': 6,\n",
    "    'G': 7,\n",
    "}\n",
    "timeMax = pd.to_datetime('1-Dec-21')\n",
    "train_data['work_year'] = train_data['work_year'].map(workYearDIc)\n",
    "test_public['work_year'] = test_public['work_year'].map(workYearDIc)\n",
    "train_data['class'] = train_data['class'].map(class_dict)\n",
    "test_public['class'] = test_public['class'].map(class_dict)\n",
    "\n",
    "train_data['earlies_credit_mon'] = pd.to_datetime(train_data['earlies_credit_mon'].map(findDig))\n",
    "test_public['earlies_credit_mon'] = pd.to_datetime(test_public['earlies_credit_mon'].map(findDig))\n",
    "train_data.loc[ train_data['earlies_credit_mon']>timeMax,'earlies_credit_mon' ] = train_data.loc[ train_data['earlies_credit_mon']>timeMax,'earlies_credit_mon' ]+  pd.offsets.DateOffset(years=-100)  \n",
    "test_public.loc[ test_public['earlies_credit_mon']>timeMax,'earlies_credit_mon' ] = test_public.loc[ test_public['earlies_credit_mon']>timeMax,'earlies_credit_mon' ]+ pd.offsets.DateOffset(years=-100)\n",
    "train_data['issue_date'] = pd.to_datetime(train_data['issue_date'])\n",
    "test_public['issue_date'] = pd.to_datetime(test_public['issue_date'])\n",
    "\n",
    "\n",
    "\n",
    "#Internet数据处理\n",
    "train_inte['work_year'] = train_inte['work_year'].map(workYearDIc)\n",
    "train_inte['class'] = train_inte['class'].map(class_dict)\n",
    "train_inte['earlies_credit_mon'] = pd.to_datetime(train_inte['earlies_credit_mon'])\n",
    "train_inte['issue_date'] = pd.to_datetime(train_inte['issue_date'])\n",
    "\n",
    "\n",
    "train_data['issue_date_month'] = train_data['issue_date'].dt.month\n",
    "test_public['issue_date_month'] = test_public['issue_date'].dt.month\n",
    "train_data['issue_date_dayofweek'] = train_data['issue_date'].dt.dayofweek\n",
    "test_public['issue_date_dayofweek'] = test_public['issue_date'].dt.dayofweek\n",
    "\n",
    "train_data['earliesCreditMon'] = train_data['earlies_credit_mon'].dt.month\n",
    "test_public['earliesCreditMon'] = test_public['earlies_credit_mon'].dt.month\n",
    "train_data['earliesCreditYear'] = train_data['earlies_credit_mon'].dt.year\n",
    "test_public['earliesCreditYear'] = test_public['earlies_credit_mon'].dt.year\n",
    "\n",
    "\n",
    "###internet数据\n",
    "\n",
    "train_inte['issue_date_month'] = train_inte['issue_date'].dt.month\n",
    "train_inte['issue_date_dayofweek'] = train_inte['issue_date'].dt.dayofweek\n",
    "train_inte['earliesCreditMon'] = train_inte['earlies_credit_mon'].dt.month\n",
    "train_inte['earliesCreditYear'] = train_inte['earlies_credit_mon'].dt.year\n",
    "\n",
    "\n",
    "cat_cols = ['employer_type', 'industry']\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "for col in cat_cols:\n",
    "    lbl = LabelEncoder().fit(train_data[col])\n",
    "    train_data[col] = lbl.transform(train_data[col])\n",
    "    test_public[col] = lbl.transform(test_public[col])\n",
    "    \n",
    "    #Internet处理\n",
    "    train_inte[col] = lbl.transform(train_inte[col])\n",
    "    \n",
    "# 'f1','policy_code','app_type' 这三个去掉是881\n",
    "# ,'f1','policy_code','app_type'\n",
    "col_to_drop = ['issue_date', 'earlies_credit_mon']\n",
    "train_data = train_data.drop(col_to_drop, axis=1)\n",
    "test_public = test_public.drop(col_to_drop, axis=1 )\n",
    "\n",
    "##internet处理\n",
    "train_inte = train_inte.drop(col_to_drop, axis=1 )\n",
    "# 暂时不变\n",
    "\n",
    "tr_cols = set(train_data.columns)\n",
    "same_col = list(tr_cols.intersection(set(train_inte.columns)))\n",
    "train_inteSame = train_inte[same_col].copy()\n",
    "\n",
    "Inte_add_cos = list(tr_cols.difference(set(same_col)))\n",
    "for col in Inte_add_cos:\n",
    "    train_inteSame[col] = np.nan\n",
    "\n",
    "\n",
    "y = train_data['isDefault']\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=546789)\n",
    "oof_preds, IntePre, importances = train_model(train_data, train_inteSame, y, folds)\n",
    "\n",
    "IntePre['isDef'] = train_inte['is_default']\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(IntePre['isDef'],IntePre.isDefault)\n",
    "## 选择阈值0.05，从internet表中提取预测小于该概率的样本，并对不同来源的样本赋予来源值\n",
    "InteId = IntePre.loc[IntePre.isDefault<0.05, 'loan_id'].tolist()\n",
    "\n",
    "train_data['dataSourse'] = 1\n",
    "test_public['dataSourse'] = 1\n",
    "train_inteSame['dataSourse'] = 0\n",
    "train_inteSame['isDefault'] = train_inte['is_default']\n",
    "use_te = train_inteSame[train_inteSame.loan_id.isin( InteId )].copy()\n",
    "data = pd.concat([ train_data,test_public,use_te]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.title(\"Distribution of Default values IntePre\")\n",
    "sns.distplot(IntePre['isDefault'],color=\"black\", kde=True,bins=120, label='train_data')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "train = data[data['isDefault'].notna()]\n",
    "test  = data[data['isDefault'].isna()]\n",
    "\n",
    "\n",
    "\n",
    "del data\n",
    "del train_data,test_public\n",
    "\n",
    "\n",
    "y = train['isDefault']\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=546789)\n",
    "oof_preds, test_preds, importances = train_model(train, test, y, folds)\n",
    "test_preds.rename({'loan_id': 'id'}, axis=1)[['id', 'isDefault']].to_csv('nn2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d62615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv('raw_data/train_public.csv')\n",
    "test_data = pd.read_csv('raw_data/test_public.csv')\n",
    "sub=pd.read_csv(\"nn2.csv\")\n",
    "sub=sub.rename(columns={'id': 'loan_id'})\n",
    "sub.loc[sub['isDefault']<0.5,'isDefault'] = 0\n",
    "nw_sub=sub[(sub['isDefault']==0)]\n",
    "nw_test_data=test_data.merge(nw_sub,on='loan_id',how='inner')\n",
    "nw_train_data = pd.concat([train_data,nw_test_data]).reset_index(drop=True)\n",
    "nw_train_data.to_csv(\"raw_data/nw_train_public.csv\",index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24729cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import re\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, average_precision_score\n",
    "from sklearn.model_selection import KFold\n",
    "from lightgbm import LGBMClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from dateutil.relativedelta import relativedelta\n",
    "train_data = pd.read_csv('./nw_train_public.csv')\n",
    "submit_example = pd.read_csv('./submit_example.csv')\n",
    "test_public = pd.read_csv('./test_public.csv')\n",
    "train_inte = pd.read_csv('./train_internet.csv')\n",
    "\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_rows', 200)\n",
    "pd.set_option('float_format', lambda x: '%.3f' % x)\n",
    "def train_model(data_, test_, y_, folds_):\n",
    "    oof_preds = np.zeros(data_.shape[0])\n",
    "    sub_preds = np.zeros(test_.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    feats = [f for f in data_.columns if f not in ['loan_id', 'user_id', 'isDefault'] ]\n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(folds_.split(data_)):\n",
    "        trn_x, trn_y = data_[feats].iloc[trn_idx], y_.iloc[trn_idx]\n",
    "        val_x, val_y = data_[feats].iloc[val_idx], y_.iloc[val_idx]\n",
    "        clf = LGBMClassifier(\n",
    "            n_estimators=4000,\n",
    "            learning_rate=0.08,\n",
    "            num_leaves=2**5,\n",
    "            colsample_bytree=.65,\n",
    "            subsample=.9,\n",
    "            max_depth=5,\n",
    "#             max_bin=250,\n",
    "            reg_alpha=.3,\n",
    "            reg_lambda=.3,\n",
    "            min_split_gain=.01,\n",
    "            min_child_weight=2,\n",
    "            silent=-1,\n",
    "            verbose=-1,\n",
    "        )\n",
    "        \n",
    "        clf.fit(trn_x, trn_y, \n",
    "                eval_set= [(trn_x, trn_y), (val_x, val_y)], \n",
    "                eval_metric='auc', verbose=100, early_stopping_rounds=40  #30\n",
    "               )\n",
    "\n",
    "        oof_preds[val_idx] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "        sub_preds += clf.predict_proba(test_[feats], num_iteration=clf.best_iteration_)[:, 1] / folds_.n_splits\n",
    "        \n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        \n",
    "        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(val_y, oof_preds[val_idx])))\n",
    "        del clf, trn_x, trn_y, val_x, val_y\n",
    "        gc.collect()\n",
    "        \n",
    "    print('Full AUC score %.6f' % roc_auc_score(y, oof_preds)) \n",
    "    \n",
    "    test_['isDefault'] = sub_preds\n",
    "\n",
    "    return oof_preds, test_[['loan_id', 'isDefault']], feature_importance_df\n",
    "    \n",
    "def display_importances(feature_importance_df_):\n",
    "    # Plot feature importances\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "        by=\"importance\", ascending=False)[:50].index\n",
    "    \n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "    \n",
    "    plt.figure(figsize=(8,10))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", \n",
    "                data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lgbm_importances.png')\n",
    "\n",
    "def workYearDIc(x):\n",
    "    if str(x)=='nan':\n",
    "        return -1\n",
    "    x = x.replace('< 1','0')\n",
    "    return int(re.search('(\\d+)', x).group())\n",
    "\n",
    "def findDig(val):\n",
    "    fd = re.search('(\\d+-)', val)\n",
    "    if fd is None:\n",
    "        return '1-'+val\n",
    "    return val + '-01'\n",
    "\n",
    "\n",
    "class_dict = {\n",
    "    'A': 1,\n",
    "    'B': 2,\n",
    "    'C': 3,\n",
    "    'D': 4,\n",
    "    'E': 5,\n",
    "    'F': 6,\n",
    "    'G': 7,\n",
    "}\n",
    "timeMax = pd.to_datetime('1-Dec-21')\n",
    "train_data['work_year'] = train_data['work_year'].map(workYearDIc)\n",
    "test_public['work_year'] = test_public['work_year'].map(workYearDIc)\n",
    "train_data['class'] = train_data['class'].map(class_dict)\n",
    "test_public['class'] = test_public['class'].map(class_dict)\n",
    "\n",
    "train_data['earlies_credit_mon'] = pd.to_datetime(train_data['earlies_credit_mon'].map(findDig))\n",
    "test_public['earlies_credit_mon'] = pd.to_datetime(test_public['earlies_credit_mon'].map(findDig))\n",
    "train_data.loc[ train_data['earlies_credit_mon']>timeMax,'earlies_credit_mon' ] = train_data.loc[ train_data['earlies_credit_mon']>timeMax,'earlies_credit_mon' ]+  pd.offsets.DateOffset(years=-100)  \n",
    "test_public.loc[ test_public['earlies_credit_mon']>timeMax,'earlies_credit_mon' ] = test_public.loc[ test_public['earlies_credit_mon']>timeMax,'earlies_credit_mon' ]+ pd.offsets.DateOffset(years=-100)\n",
    "train_data['issue_date'] = pd.to_datetime(train_data['issue_date'])\n",
    "test_public['issue_date'] = pd.to_datetime(test_public['issue_date'])\n",
    "\n",
    "\n",
    "\n",
    "#Internet数据处理\n",
    "train_inte['work_year'] = train_inte['work_year'].map(workYearDIc)\n",
    "train_inte['class'] = train_inte['class'].map(class_dict)\n",
    "train_inte['earlies_credit_mon'] = pd.to_datetime(train_inte['earlies_credit_mon'])\n",
    "train_inte['issue_date'] = pd.to_datetime(train_inte['issue_date'])\n",
    "\n",
    "\n",
    "train_data['issue_date_month'] = train_data['issue_date'].dt.month\n",
    "test_public['issue_date_month'] = test_public['issue_date'].dt.month\n",
    "train_data['issue_date_dayofweek'] = train_data['issue_date'].dt.dayofweek\n",
    "test_public['issue_date_dayofweek'] = test_public['issue_date'].dt.dayofweek\n",
    "\n",
    "train_data['earliesCreditMon'] = train_data['earlies_credit_mon'].dt.month\n",
    "test_public['earliesCreditMon'] = test_public['earlies_credit_mon'].dt.month\n",
    "train_data['earliesCreditYear'] = train_data['earlies_credit_mon'].dt.year\n",
    "test_public['earliesCreditYear'] = test_public['earlies_credit_mon'].dt.year\n",
    "\n",
    "\n",
    "###internet数据\n",
    "\n",
    "train_inte['issue_date_month'] = train_inte['issue_date'].dt.month\n",
    "train_inte['issue_date_dayofweek'] = train_inte['issue_date'].dt.dayofweek\n",
    "train_inte['earliesCreditMon'] = train_inte['earlies_credit_mon'].dt.month\n",
    "train_inte['earliesCreditYear'] = train_inte['earlies_credit_mon'].dt.year\n",
    "\n",
    "\n",
    "cat_cols = ['employer_type', 'industry']\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "for col in cat_cols:\n",
    "    lbl = LabelEncoder().fit(train_data[col])\n",
    "    train_data[col] = lbl.transform(train_data[col])\n",
    "    test_public[col] = lbl.transform(test_public[col])\n",
    "    \n",
    "    #Internet处理\n",
    "    train_inte[col] = lbl.transform(train_inte[col])\n",
    "    \n",
    "# 'f1','policy_code','app_type' 这三个去掉是881\n",
    "# ,'f1','policy_code','app_type'\n",
    "col_to_drop = ['issue_date', 'earlies_credit_mon']\n",
    "train_data = train_data.drop(col_to_drop, axis=1)\n",
    "test_public = test_public.drop(col_to_drop, axis=1 )\n",
    "\n",
    "##internet处理\n",
    "train_inte = train_inte.drop(col_to_drop, axis=1 )\n",
    "# 暂时不变\n",
    "\n",
    "tr_cols = set(train_data.columns)\n",
    "same_col = list(tr_cols.intersection(set(train_inte.columns)))\n",
    "train_inteSame = train_inte[same_col].copy()\n",
    "\n",
    "Inte_add_cos = list(tr_cols.difference(set(same_col)))\n",
    "for col in Inte_add_cos:\n",
    "    train_inteSame[col] = np.nan\n",
    "\n",
    "#81后加 \n",
    "\n",
    "y = train_data['isDefault']\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=546789)\n",
    "oof_preds, IntePre, importances = train_model(train_data, train_inteSame, y, folds)\n",
    "\n",
    "IntePre['isDef'] = train_inte['is_default']\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(IntePre['isDef'],IntePre.isDefault)\n",
    "## 选择阈值0.05，从internet表中提取预测小于该概率的样本，并对不同来源的样本赋予来源值\n",
    "InteId = IntePre.loc[IntePre.isDefault<0.5, 'loan_id'].tolist()\n",
    "\n",
    "train_data['dataSourse'] = 1\n",
    "test_public['dataSourse'] = 1\n",
    "train_inteSame['dataSourse'] = 0\n",
    "train_inteSame['isDefault'] = train_inte['is_default']\n",
    "use_te = train_inteSame[train_inteSame.loan_id.isin( InteId )].copy()\n",
    "data = pd.concat([ train_data,test_public,use_te]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.title(\"Distribution of Default values IntePre\")\n",
    "sns.distplot(IntePre['isDefault'],color=\"black\", kde=True,bins=120, label='train_data')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "train = data[data['isDefault'].notna()]\n",
    "test  = data[data['isDefault'].isna()]\n",
    "\n",
    "\n",
    "del data\n",
    "del train_data,test_public\n",
    "\n",
    "\n",
    "y = train['isDefault']\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=546789)\n",
    "oof_preds, test_preds, importances = train_model(train, test, y, folds)\n",
    "test_preds.rename({'loan_id': 'id'}, axis=1)[['id', 'isDefault']].to_csv('baseline891.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
